{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A guide to Pandas.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "MJnFbEKFFbBr",
        "e6gG583iOVTw",
        "usVtRW1TNmkj",
        "X0RYVly1klhE",
        "u8FvY2vgi6dD",
        "Zkr-9YKNkGut"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNbZIMwMzQtwC8Z+joMkSPU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenHigginsData/pythonguides/blob/main/A_guide_to_Pandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0zxARcm-Ygk"
      },
      "source": [
        "# <u>**A guide to Pandas**</u>\n",
        "\n",
        "The prupose of this notbook is to act as a referral for all your Pandas data transformation needs.\n",
        "\n",
        "The dataset used for this notebook can be found here: *put a hyperlink here* \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55YJsxDaDBcY"
      },
      "source": [
        "# Prerequisites\n",
        "\n",
        "Please run the cell directly below before continuing. The cell will load all necessary dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5VX35gHC_1G"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Pandas documentation: https://pandas.pydata.org/pandas-docs/stable/index.html"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAK8RfnU_42S"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsGd4X2H-TcI"
      },
      "source": [
        "# To get data located in github click on the file and view raw. Copy the url from the page that loads.\n",
        "football_dataset_gh = 'https://raw.githubusercontent.com/BenHigginsData/pythonguides/main/Football%20Results%201872-2021.csv'\n",
        "weather_dataset_gh = 'https://raw.githubusercontent.com/BenHigginsData/pythonguides/main/Data/England%20and%20Wales%20daily%20precip.csv'\n",
        "\n",
        "sales_electrical_gh = 'https://raw.githubusercontent.com/BenHigginsData/pythonguides/main/Data/Sales%20-%20Electrical%20Products.csv'\n",
        "sales_garden_gh = 'https://raw.githubusercontent.com/BenHigginsData/pythonguides/main/Data/Sales%20-%20Garden%20Products.csv'\n",
        "sales_kitchen_gh = 'https://raw.githubusercontent.com/BenHigginsData/pythonguides/main/Data/Sales%20-%20Kitchen%20Products.csv'\n",
        "sales_lookup = 'https://raw.githubusercontent.com/BenHigginsData/pythonguides/main/Data/Sales%20lookup.csv'\n",
        "\n",
        "df = pd.read_csv(football_dataset_gh)\n",
        "df_weather = pd.read_csv(weather_dataset_gh)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31KFeJBYCOmA"
      },
      "source": [
        "Other arguments within the pd.read_FILETYPE function that may be useful: ***Argument=*** Description\n",
        "\n",
        "***header=*** Row number(s) to use as the column names, and the start of the data. <br>\n",
        "***usecols=*** Return a subset of the columns. Specify with a list of numbers or column names, can use column letters with pd.read_excel. <br>\n",
        "***index_col=*** Specify which column to use as an index.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJnFbEKFFbBr"
      },
      "source": [
        "# Basic dataset information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBuVnkNhFWVs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "dda87c7a-4cc5-47b7-e916-df49ffd44349"
      },
      "source": [
        "# View the first few rows of the dataset:\n",
        "# by default the first 5 rows are shown; pass an integer as the argument to change the number of rows shown. \n",
        "# Passing a -ve number as the argument will show all but the bottom n rows.\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>home_team</th>\n",
              "      <th>away_team</th>\n",
              "      <th>home_score</th>\n",
              "      <th>away_score</th>\n",
              "      <th>tournament</th>\n",
              "      <th>city</th>\n",
              "      <th>country</th>\n",
              "      <th>neutral</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1872-11-30</td>\n",
              "      <td>Scotland</td>\n",
              "      <td>England</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Friendly</td>\n",
              "      <td>Glasgow</td>\n",
              "      <td>Scotland</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1873-03-08</td>\n",
              "      <td>England</td>\n",
              "      <td>Scotland</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>Friendly</td>\n",
              "      <td>London</td>\n",
              "      <td>England</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1874-03-07</td>\n",
              "      <td>Scotland</td>\n",
              "      <td>England</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>Friendly</td>\n",
              "      <td>Glasgow</td>\n",
              "      <td>Scotland</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1875-03-06</td>\n",
              "      <td>England</td>\n",
              "      <td>Scotland</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>Friendly</td>\n",
              "      <td>London</td>\n",
              "      <td>England</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1876-03-04</td>\n",
              "      <td>Scotland</td>\n",
              "      <td>England</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>Friendly</td>\n",
              "      <td>Glasgow</td>\n",
              "      <td>Scotland</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         date home_team away_team  ...     city   country neutral\n",
              "0  1872-11-30  Scotland   England  ...  Glasgow  Scotland   False\n",
              "1  1873-03-08   England  Scotland  ...   London   England   False\n",
              "2  1874-03-07  Scotland   England  ...  Glasgow  Scotland   False\n",
              "3  1875-03-06   England  Scotland  ...   London   England   False\n",
              "4  1876-03-04  Scotland   England  ...  Glasgow  Scotland   False\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDXphlJ4FWXo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11388cbf-310e-4351-c951-729b55e044a3"
      },
      "source": [
        "# Information on each of the columns, such as the data type and number of missing values.\n",
        "\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 42483 entries, 0 to 42482\n",
            "Data columns (total 9 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   date        42483 non-null  object\n",
            " 1   home_team   42483 non-null  object\n",
            " 2   away_team   42483 non-null  object\n",
            " 3   home_score  42483 non-null  int64 \n",
            " 4   away_score  42483 non-null  int64 \n",
            " 5   tournament  42483 non-null  object\n",
            " 6   city        42483 non-null  object\n",
            " 7   country     42483 non-null  object\n",
            " 8   neutral     42483 non-null  bool  \n",
            "dtypes: bool(1), int64(2), object(6)\n",
            "memory usage: 2.6+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNOBxHS8FWaz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "834945f7-9c18-401f-bc58-a6cb077339d8"
      },
      "source": [
        "# The number of rows and columns in the dataset\n",
        "\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42483, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn2vpw2fFWcq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3702f047-c9da-49ac-c80a-9ecebebaeb98"
      },
      "source": [
        "# df.values creates a numpy array of all the values with each row as a list\n",
        "\n",
        "df.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['1872-11-30', 'Scotland', 'England', ..., 'Glasgow', 'Scotland',\n",
              "        False],\n",
              "       ['1873-03-08', 'England', 'Scotland', ..., 'London', 'England',\n",
              "        False],\n",
              "       ['1874-03-07', 'Scotland', 'England', ..., 'Glasgow', 'Scotland',\n",
              "        False],\n",
              "       ...,\n",
              "       ['2021-07-29', 'Mexico', 'Canada', ..., 'Houston',\n",
              "        'United States', True],\n",
              "       ['2021-07-29', 'United States', 'Qatar', ..., 'Austin',\n",
              "        'United States', False],\n",
              "       ['2021-08-01', 'United States', 'Mexico', ..., 'Las Vegas',\n",
              "        'United States', False]], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cy3xZzWUFWej",
        "outputId": "78ef9c09-25a8-4891-9713-3fe192c18304"
      },
      "source": [
        "# Column names\n",
        "\n",
        "df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['date', 'home_team', 'away_team', 'home_score', 'away_score',\n",
              "       'tournament', 'city', 'country', 'neutral'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVZQsEbWKmXH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "13e3a8c7-e6b0-48a8-fa00-e461ee5804ab"
      },
      "source": [
        "# Summary statistics\n",
        "\n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>home_score</th>\n",
              "      <th>away_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>42483.000000</td>\n",
              "      <td>42483.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.743827</td>\n",
              "      <td>1.185557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.752594</td>\n",
              "      <td>1.403908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>31.000000</td>\n",
              "      <td>21.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         home_score    away_score\n",
              "count  42483.000000  42483.000000\n",
              "mean       1.743827      1.185557\n",
              "std        1.752594      1.403908\n",
              "min        0.000000      0.000000\n",
              "25%        1.000000      0.000000\n",
              "50%        1.000000      1.000000\n",
              "75%        2.000000      2.000000\n",
              "max       31.000000     21.000000"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B08Gbr0sKmZG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7131470-3aa2-4319-d2b9-cdeb3618e8df"
      },
      "source": [
        "# Summary of missing values per column\n",
        "\n",
        "df.isna().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "date          0\n",
              "home_team     0\n",
              "away_team     0\n",
              "home_score    0\n",
              "away_score    0\n",
              "tournament    0\n",
              "city          0\n",
              "country       0\n",
              "neutral       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlLio-vTKmbP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f4790dcb-a00f-48c8-e0c3-593f59d3bb68"
      },
      "source": [
        "# Unique values\n",
        "df['city'].unique()\n",
        "\n",
        "# We then get the length of the list to get a count of unique values\n",
        "display(len(df['city'].unique()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "2050"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7BO-AwSVrzD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7866d4f1-f458-46c6-f831-c675ff902057"
      },
      "source": [
        "# Count of values per unique category entry\n",
        "\n",
        "df['city'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Kuala Lumpur    589\n",
              "Doha            474\n",
              "Bangkok         427\n",
              "London          413\n",
              "Budapest        398\n",
              "               ... \n",
              "Seongnam          1\n",
              "Kruševac          1\n",
              "Parchal           1\n",
              "Urziceni          1\n",
              "Chillán           1\n",
              "Name: city, Length: 2050, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZVJlg-oMOc2"
      },
      "source": [
        "# Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6gG583iOVTw"
      },
      "source": [
        "## Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNdln_-0OYEQ"
      },
      "source": [
        "# Remove duplicate rows\n",
        "\n",
        "df_no_dups =df.drop_duplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-_ge2C-ijwV"
      },
      "source": [
        "# Remove rows with missing data/NAs (use the 'subset=' argument to specify which columns to check)\n",
        "\n",
        "df_no_missing = df.dropna(subset = ['country', 'tournament'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzLc03D0hzwM"
      },
      "source": [
        "# Replace missing/NA values:\n",
        "\n",
        "df.fillna(\"data is missing\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hp6knnH6Qr_Z"
      },
      "source": [
        "# Remove certain characters from certain columns\n",
        "\n",
        "\n",
        "# List of characters to remove\n",
        "chars_to_remove = [\"+\", \",\", \"$\"]\n",
        "\n",
        "# List of column names to clean\n",
        "cols_to_clean = [\"tournament\", \"city\"]\n",
        "\n",
        "# Loop for replacing the characters with an empty string on the specified columns.\n",
        "# for each column --> for each character --> replace the column's values with the character remove.\n",
        "for col in cols_to_clean:\n",
        "    for char in chars_to_remove:\n",
        "        df[col] = df[col].apply(lambda value: value.replace(char, \"\"))\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNVWWKE_Skxx"
      },
      "source": [
        "# Correcting data types with 'df.astype()'\n",
        "\n",
        "df[\"home_score\"] = df[\"home_score\"].astype(\"int\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1E_a2NsWJeH"
      },
      "source": [
        "# Rename columns:\n",
        "\n",
        "# needs a ditionary of {old name: new name, old name: new name, .....}\n",
        "df.rename(columns={'home_score': 'home goals', 'away_score': 'away goals'}, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5221I10e_zI"
      },
      "source": [
        "# Setting a column to index\n",
        "\n",
        "df_indexed = df.set_index('country')\n",
        "\n",
        "# Can set nested indexes:\n",
        "\n",
        "df_nest_indexed= df.set_index([\"country\", \"city\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwHjALOufYM0"
      },
      "source": [
        "# Resetting the index\n",
        "\n",
        "# pass 'drop=True' to the function and the index column will be removed.\n",
        "df_reset_ind = df.reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyQIAZa4mQNS"
      },
      "source": [
        "# The df.melt() method allows us to unpivot data from wide to long format."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usVtRW1TNmkj"
      },
      "source": [
        "## Sorting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI_wPLwrKmdL"
      },
      "source": [
        "# Sort on a single column, descending by default\n",
        "df.sort_values(\"country\")\n",
        "\n",
        "# Sort on multiple columns with multiple ascending/descending conditions\n",
        "df.sort_values([\"country\", \"city\"], ascending=[False, True])\n",
        "\n",
        "# Sort by the row index\n",
        "df.sort_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L71W57jYZejT"
      },
      "source": [
        "## Subsetting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0RYVly1klhE"
      },
      "source": [
        "### Subsetting with conditions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFcWYnfabP67"
      },
      "source": [
        "# Subsetting a dataset to fewer columns:\n",
        "\n",
        "df_3cols = df[['country', 'city', 'date']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut2QA-0yckAl"
      },
      "source": [
        "# Conditional subsetting\n",
        "\n",
        "df_England = df[df['country'] == 'England']\n",
        "\n",
        "# use multiple criteria with 'and &' or 'or |' by bracketing the different conditions\n",
        "df_high_goal_count = df[(df['home_score'] > 10) | (df['away_score'] > 10)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMzv1T9qddYy"
      },
      "source": [
        "# Subsetting on multiple criteria in one column\n",
        "my_list = ['England', 'Scotland', 'Wales']\n",
        "df_uk = df[df.country.isin(my_list)]\n",
        "df_uk['country'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqUhMo9FMSYu"
      },
      "source": [
        "# Selecting rows where certain columns are not blank.\n",
        "\n",
        "df_non_blank = df[(~df['city'].isnull()) & (~df['tournament'].isnull())]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69-CkpwGbP4g"
      },
      "source": [
        "# Subset categories with a minimum amount of observations\n",
        "df_most_games = df.groupby(['city']).filter(lambda x: len(x) >= 250)\n",
        "\n",
        "# Confirm with:\n",
        "df_most_games['city'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMvAAsmEkrnY"
      },
      "source": [
        "### Subset using df.query()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxJtqdDHbP8_"
      },
      "source": [
        "# Subset with Query:\n",
        "\n",
        "df_q = df.query(\"country=='Wales' or (city =='London' and home_score > 5)\")\n",
        "df_q.sort_values(by='home_score', ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyjk3CL5kxHk"
      },
      "source": [
        "### Subset using df.loc and df.iloc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sEKhBYaipUg"
      },
      "source": [
        "# df.loc (location)\n",
        "\n",
        "# Can slice rows if the index is unique:\n",
        "df.loc[10:15]\n",
        "\n",
        "# If there are nested indexes you can slice with tuples:\n",
        "df.loc[(\"outer_row_1\", \"outer_row_n\"):(\"inner_row_1\", \"inner_row_n\")]\n",
        "\n",
        "# Can slice columns:\n",
        "df.loc[:, \"country\":\"neutral\"]\n",
        "\n",
        "# can slice rows and columns at the same time:\n",
        "df_sorted.loc[(\"outer_row_1\", \"outer_row_n\"):(\"inner_row_1\", \"inner_row_n\"),\"date\":\"country\"]\n",
        "\n",
        "# slice dates (might have to be the index)\n",
        "df.loc[\"2014-08-25\":\"2016-09-16\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42c2qcFMiqBP"
      },
      "source": [
        "# df.iloc (index location)\n",
        "\n",
        "# df.iloc[row slice, column slice]\n",
        "df.iloc[2:5, 1:4]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8FvY2vgi6dD"
      },
      "source": [
        "## New columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGYrGeA5bP_e"
      },
      "source": [
        "# Add a new column\n",
        "\n",
        "df['home score mean'] = df['home_score'].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCEB-GXvjnWH"
      },
      "source": [
        "# Converting date to a datetime type and adding different date related columns\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "df['year'] = df['date'].dt.year\n",
        "df['month'] = df['date'].dt.month\n",
        "df['day'] = df['date'].dt.day\n",
        "df['weekday'] = df['date'].dt.day_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XewDu3IEYqK_"
      },
      "source": [
        "# Min and max date\n",
        "display(df[\"date\"].min())\n",
        "display(df[\"date\"].max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3gw8V45f1ly"
      },
      "source": [
        "# Getting the decade (same method for bucketing)\n",
        "\n",
        "df['decade'] = (np.floor(df['year'] / 10) * 10).astype(int)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rmK57OEkI7y"
      },
      "source": [
        "# Adding a calculated column\n",
        "df['home_goal_difference'] = df['home_score'] - df['away_score']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zkr-9YKNkGut"
      },
      "source": [
        "# Stats and calculations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVfOIkPvXoT4"
      },
      "source": [
        "# the agg() function for multiple calculations:\n",
        "\n",
        "# e.g. function for IQR\n",
        "def iqr(column):\n",
        "    return column.quantile(0.75) - column.quantile(0.25)\n",
        "\n",
        "\n",
        "df_calcs1 = df[[\"home_score\", \"away_score\"]].agg([iqr, np.median, np.mean])\n",
        "df_calcs1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptR-GbkolfiG"
      },
      "source": [
        "# Summarise by category value with groupby()\n",
        "\n",
        "df_country_mean = df.groupby([\"country\",'decade'])[\"away_score\"].mean()\n",
        "df_country_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STo9w5kubJAL"
      },
      "source": [
        "# Pivot table\n",
        "\n",
        "df_pivot = df.pivot_table(values=\"home_score\",\n",
        "                         index=\"country\",\n",
        "                         columns=\"neutral\",\n",
        "                         fill_value=\"No games\",\n",
        "                         margins=True)\n",
        "\n",
        "display(df_pivot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRUWmrxtghMd"
      },
      "source": [
        "# Getting the smallest and largest values:\n",
        "\n",
        "display(df[df['country'] == 'Germany'].nsmallest(1, 'year'))\n",
        "display(df.nlargest(3, 'away_score'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQ5w6YFkR1tV"
      },
      "source": [
        "# Joining datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diDijDkWpLRZ"
      },
      "source": [
        "To practice with the merging functions we can first load a few small tables of fake data to dow with sales in a homeware store."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8StcdFc6nBIX"
      },
      "source": [
        "sales_electrical_gh\n",
        "sales_garden_gh\n",
        "sales_kitchen_gh\n",
        "\n",
        "df_elec_sales = pd.read_csv(sales_electrical_gh)\n",
        "df_garden_sales = pd.read_csv(sales_garden_gh)\n",
        "df_kitchen_sales = pd.read_csv(sales_kitchen_gh)\n",
        "df_sales_lookup = pd.read_csv(sales_lookup)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU9gbg9_7wud"
      },
      "source": [
        "## Concatenating (Vertical join)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NQ7M4XrlP3h"
      },
      "source": [
        "# Concatenate data (joining vertically)\n",
        "\n",
        "# Let's vertically join the three sales tables together using pd.concat().\n",
        "df_all_sales = pd.concat([df_elec_sales, df_garden_sales, df_kitchen_sales], ignore_index=False, keys=[\"elc\", \"gdn\", \"ktn\"])\n",
        "\n",
        "# ignore_index=True will create an index that spans the whole table, False will keep the index values of each separate table (False by default).\n",
        "# keys= allows us to create an outer index, which might be good to keep track of where each row came from.\n",
        "# sort=True sorts the columns alphabetically when concatenating\n",
        "\n",
        "# pd.concat() will automatically include all the columns and put NaN against the table with fewer columns if joining tables with different amount of columns.\n",
        "# To only keep the columns that the tables have in common, set the 'join=' argument to \"inner\". pb.concat() is an outer join by default\n",
        "\n",
        "\n",
        "df_all_sales"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njU4GyaSlPqs"
      },
      "source": [
        "# Append data (joining vertically)\n",
        "\n",
        "# df.append() is a simplified version of pd.concat()\n",
        "# You can use ignore_index= and sort=, but not keys= and join=. df.append is always an outer join.\n",
        "# Note, we call this as a function of a dataframe object, therefore one table goes before .append and the rest in the list:\n",
        "\n",
        "df_appended = df_elec_sales.append([df_garden_sales, df_kitchen_sales])\n",
        "\n",
        "df_appended"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2z_3m1dlPlU"
      },
      "source": [
        "# df.merge()\n",
        "# Inner, Outer, Left, Right\n",
        "# one/many to many/one\n",
        "# merge using the index column"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kobstjJd7_ci"
      },
      "source": [
        "## Merging (Horizontal join)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2qad3bm8Po-"
      },
      "source": [
        "### Inner join\n",
        "\n",
        "Inner joins only return the rows with matching values in both tables. Any rows without a matching counterpart will be dropped."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKwz_D9K009T"
      },
      "source": [
        "# Joining data with df.merge()\n",
        "\n",
        "# df.merge() is an inner join by default. You can merge on more than one column (e.g. hotel and date) by passing a list to the on= argument.\n",
        "\n",
        "df_inner = df_all_sales.merge(df_sales_lookup, on='Product ID', suffixes=(\"_AS\",\"_SLU\"))\n",
        "\n",
        "# In this example both tables have a column called 'Product ID'. If the column names are different but can be joined on because they have the same values, we can \n",
        "# specify which columns to join on each table with 'left_on=' and 'right_on='. E.g. \"left_on='Postcode'\", \"right_on='post_code'\".\n",
        "\n",
        "\n",
        "display(df_inner)\n",
        "\n",
        "# Let's check to see how many rows remaining from the inner join\n",
        "t1 = df_all_sales.shape\n",
        "t2 = df_sales_lookup.shape\n",
        "t3 = df_inner.shape\n",
        "print(f\"df_all_sales no rows:     {t1[0]}\")\n",
        "print(f\"df_sales lookup no rows:  {t2[0]}\")\n",
        "print(f\"max no rows:              {np.maximum(t1[0], t2[0])}\")\n",
        "\n",
        "print(f\"\\ndf_inner no rows:         {t3[0]}\")\n",
        "print(f\"No rows lost:             {t3[0] - np.maximum(t1[0], t2[0])}\")\n",
        "\n",
        "\n",
        "# We can see which entries do not have a match to the main data set by filtering on the lookup table\n",
        "df_not_in_all = df_sales_lookup[~df_sales_lookup['Product ID'].isin(df_all_sales['Product ID'])]\n",
        "df_not_in_all\n",
        "\n",
        "# We can merge merged tables in one execution by adding another .merged() command to the end of the the first one:\n",
        "# e.g. df_master = df1.merge(df2, on=col1).merge(df3, on=col2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxe-8FoR8m6s"
      },
      "source": [
        "### Left join\n",
        "Left join contains all rows from the left table and only content from the right table where rows match *(Keep everything on the left, join only the same rows from the right)*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U86LPiSQ-wId"
      },
      "source": [
        "# Left join example"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPGFQfYV8nF8"
      },
      "source": [
        "### Right join\n",
        "\n",
        "Opposite of a left join - keeps all of the right table and only the content from the left table where the rows match."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDxwW89u-rJg"
      },
      "source": [
        "# Right join example"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg7G7bxk8nJH"
      },
      "source": [
        "### Outer join\n",
        "Every row is returned from both tables, and missing data is NaN in non matching rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkvMmyhC-zJV"
      },
      "source": [
        "# Outer join example"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kZIeLEV_Oa1"
      },
      "source": [
        "### Self join\n",
        "We can use this when we want to add an entry from a different row into a column next to another row. <br>\n",
        "*E.g. a table of movies: 'Film' = Toy Story <--- use this same column to populate the column 'Sequel' = Toy Story 2*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOzmV_WX_pxk"
      },
      "source": [
        "# Example of a self join"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5ffbPhJ8xg8"
      },
      "source": [
        "### Semi-join"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2UbYVBi-1Rt"
      },
      "source": [
        "# Semi join example"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s02q6QL_8xjE"
      },
      "source": [
        "### Anti-join"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69vwaqrH-1y2"
      },
      "source": [
        "# Anti join example"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfJA8sbf8xlQ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D603JJOlPot"
      },
      "source": [
        "# filtering joins\n",
        "# semi-join \n",
        "# anti-join\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC_uLalTbQZd"
      },
      "source": [
        "# Other"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39C5zUiLKxsZ"
      },
      "source": [
        "# Binning data\n",
        "# df['price_category'] = pd.cut(df.price, [-np.inf, 400, 1000, np.inf],\n",
        "                              # labels=['low', 'medium', 'high'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGxIcKNHRxir"
      },
      "source": [
        "# A short analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5Iddk9IWfXt"
      },
      "source": [
        "# Organise data to tidy format with a day number\n",
        "# Add a column for country\n",
        "# Append the weather data together\n",
        "# Join the weather data to the football data\n",
        "# Subset the data to 'has weather data' for weather analysis.\n",
        "# Find the teams, home cites, and away cities that win most games in the wet, dry, and snow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Gnl8R9cmAXn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}